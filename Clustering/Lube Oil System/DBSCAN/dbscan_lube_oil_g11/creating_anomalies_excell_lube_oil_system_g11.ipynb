{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل از مسیر:\n",
      "C:\\BI\\lube_oil_system_data_g11.xlsx\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا):\n",
      "['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد رکوردهای آماده برای مدل: 10319\n",
      "در حال اجرای الگوریتم DBSCAN...\n",
      "تعداد خوشه‌های تشکیل شده: 13\n",
      "تعداد انومالی‌های تشخیص داده شده: 482 رکورد (4.67%)\n",
      "\n",
      "در حال ذخیره فایل انومالی‌ها...\n",
      "انومالی‌ها با موفقیت ذخیره شدند!\n",
      "تعداد انومالی‌ها: 482 ردیف\n",
      "مسیر فایل خروجی:\n",
      "C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"   # مسیر اصلی\n",
    "\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "# بررسی وجود فایل ورودی\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل پیدا نشد!\\nمسیر مورد نظر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5           # می‌تونی بعداً بین 0.3 تا 1.0 تست کنی\n",
    "min_samples = 10    # حداقل نقاط برای تشکیل هسته خوشه\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل از مسیر:\")\n",
    "print(input_file)\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# پیدا کردن ستون‌های سنسور (آن‌هایی که با AssetID_ شروع می‌شن و با عدد تمام می‌شن)\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا):\")\n",
    "print(sensor_columns)\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"هیچ ستون سنسوری با نام AssetID_XXXX پیدا نشد!\")\n",
    "\n",
    "# ستون‌های شناسه و زمانی که می‌خواهیم در خروجی نگه داریمid, RecordDate, RecordTime\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش داده‌های سنسور -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "\n",
    "# پر کردن مقادیر گمشده (DBSCAN با NaN کار نمی‌کنه)\n",
    "X = X.fillna(X.mean())          # می‌تونی به X.median() یا استراتژی دیگه تغییر بدی\n",
    "\n",
    "# استانداردسازی (خیلی مهم برای DBSCAN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد رکوردهای آماده برای مدل: {X_scaled.shape[0]}\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(\"در حال اجرای الگوریتم DBSCAN...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# آمار سریع\n",
    "n_anomalies = np.sum(labels == -1)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(f\"تعداد خوشه‌های تشکیل شده: {n_clusters}\")\n",
    "print(f\"تعداد انومالی‌های تشخیص داده شده: {n_anomalies} رکورد ({n_anomalies/len(labels)*100:.2f}%)\")\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها -------------------\n",
    "anomaly_indices = df.index[labels == -1]  # ایندکس‌های اصلی در دیتافریم اصلی\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "\n",
    "# اضافه کردن برچسب DBSCAN (همه -1 هستند)\n",
    "anomalies_df['DBSCAN_Label'] = -1\n",
    "\n",
    "# مرتب‌سازی بر اساس تاریخ و زمان (اگر موجود باشد)\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n",
    "else:\n",
    "    anomalies_df = anomalies_df.sort_index()\n",
    "\n",
    "# ------------------- ذخیره فایل انومالی در مسیر C:\\BI -------------------\n",
    "print(\"\\nدر حال ذخیره فایل انومالی‌ها...\")\n",
    "anomalies_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"انومالی‌ها با موفقیت ذخیره شدند!\")\n",
    "print(f\"تعداد انومالی‌ها: {len(anomalies_df)} ردیف\")\n",
    "print(f\"مسیر فایل خروجی:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل اکسل...\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا): ['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد کل رکوردهای آماده برای خوشه‌بندی: 10,319 رکورد\n",
      "\n",
      "اجرای DBSCAN با eps=0.5 و min_samples=10...\n",
      "\n",
      "============================================================\n",
      "               خلاصه نتایج خوشه‌بندی DBSCAN\n",
      "============================================================\n",
      "تعداد کل رکورد ها        : 10,319\n",
      "تعداد انومالی (نویز)     : 482  (  4.67%)\n",
      "تعداد خوشه‌های اصلی      : 13\n",
      "------------------------------------------------------------\n",
      "خوشه       تعداد اعضا      درصد از کل\n",
      "------------------------------------------------------------\n",
      "خوشه 0           4,308 رکورد    ( 41.75%)\n",
      "خوشه 6           2,789 رکورد    ( 27.03%)\n",
      "خوشه 2           1,380 رکورد    ( 13.37%)\n",
      "خوشه 12          1,211 رکورد    ( 11.74%)\n",
      "خوشه 9              29 رکورد    (  0.28%)\n",
      "خوشه 1              22 رکورد    (  0.21%)\n",
      "خوشه 4              19 رکورد    (  0.18%)\n",
      "خوشه 7              16 رکورد    (  0.16%)\n",
      "خوشه 11             16 رکورد    (  0.16%)\n",
      "خوشه 8              14 رکورد    (  0.14%)\n",
      "خوشه 5              13 رکورد    (  0.13%)\n",
      "خوشه 10             13 رکورد    (  0.13%)\n",
      "خوشه 3               7 رکورد    (  0.07%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "# بررسی وجود فایل ورودی\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل ورودی پیدا نشد!\\nمسیر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل اکسل...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا): {sensor_columns}\")\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"ستون سنسوری پیدا نشد!\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد کل رکوردهای آماده برای خوشه‌بندی: {len(X_scaled):,} رکورد\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(f\"\\nاجرای DBSCAN با eps={eps} و min_samples={min_samples}...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- آمار خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "n_clusters = len(label_counts) - (1 if -1 in label_counts else 0)\n",
    "n_noise = label_counts[-1] if -1 in label_counts else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"               خلاصه نتایج خوشه‌بندی DBSCAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'تعداد کل رکورد ها':<25}: {len(labels):,}\")\n",
    "print(f\"{'تعداد انومالی (نویز)':<25}: {n_noise:,}  ({n_noise/len(labels)*100:6.2f}%)\")\n",
    "print(f\"{'تعداد خوشه‌های اصلی':<25}: {n_clusters:,}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# نمایش تعداد اعضا در هر خوشه (به ترتیب اندازه)\n",
    "cluster_sizes = []\n",
    "for label, count in label_counts.items():\n",
    "    if label != -1:  # فقط خوشه‌های واقعی\n",
    "        cluster_sizes.append((label, count))\n",
    "\n",
    "# مرتب‌سازی بر اساس تعداد اعضا (نزولی)\n",
    "cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'خوشه':<10} {'تعداد اعضا':<15} {'درصد از کل'}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (label, count) in enumerate(cluster_sizes, 1):\n",
    "    percentage = count / len(labels) * 100\n",
    "    print(f\"خوشه {label:<6} {count:>10,} رکورد    ({percentage:6.2f}%)\")\n",
    "    if i >= 20:  # فقط 20 خوشه بزرگ را نشان بده تا شلوغ نشود\n",
    "        if len(cluster_sizes) > 20:\n",
    "            print(f\"    ... و {len(cluster_sizes) - 20} خوشه کوچک دیگر\")\n",
    "        break\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها -------------------\n",
    "anomaly_mask = labels == -1\n",
    "anomaly_indices = df.index[anomaly_mask]\n",
    "\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "anomalies_df['DBSCAN_Label'] = -1\n",
    "anomalies_df['Cluster_Size'] = n_noise  # فقط برای اطلاعات\n",
    "\n",
    "# مرتب‌سازی بر اساس تاریخ و زمان\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل اکسل...\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا): ['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد کل رکوردهای آماده: 10,319 رکورد\n",
      "\n",
      "اجرای DBSCAN با eps=0.5 و min_samples=10...\n",
      "\n",
      "======================================================================\n",
      "                  خلاصه نتایج نهایی (با قوانین جدید)\n",
      "======================================================================\n",
      "کل داده‌ها                    : 10,319\n",
      "نویز DBSCAN (برچسب -1)        : 482 رکورد\n",
      "خوشه‌های کوچک (< {MIN_CLUSTER_SIZE} عضو): 4 خوشه → 47 رکورد\n",
      "مجموع انومالی‌ها              : 529 رکورد ( 5.13%)\n",
      "خوشه‌های اصلی (عادی ≥ {MIN_CLUSTER_SIZE} عضو): 9 خوشه\n",
      "----------------------------------------------------------------------\n",
      "خوشه اصلی    تعداد اعضا      درصد از کل\n",
      "----------------------------------------------------------------------\n",
      "خوشه 0             4,308 رکورد    ( 41.75%)\n",
      "خوشه 6             2,789 رکورد    ( 27.03%)\n",
      "خوشه 2             1,380 رکورد    ( 13.37%)\n",
      "خوشه 12            1,211 رکورد    ( 11.74%)\n",
      "خوشه 9                29 رکورد    (  0.28%)\n",
      "خوشه 1                22 رکورد    (  0.21%)\n",
      "خوشه 4                19 رکورد    (  0.18%)\n",
      "خوشه 7                16 رکورد    (  0.16%)\n",
      "خوشه 11               16 رکورد    (  0.16%)\n",
      "======================================================================\n",
      "\n",
      "فایل انومالی‌ها با موفقیت ذخیره شد!\n",
      "   تعداد رکوردهای انومالی (شامل نویز و خوشه‌های کوچک): 529 رکورد\n",
      "   مسیر فایل: C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n",
      "\n",
      "کار با موفقیت به پایان رسید!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل ورودی پیدا نشد!\\nمسیر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "MIN_CLUSTER_SIZE = 15  # حداقل اندازه خوشه برای \"عادی\" در نظر گرفتن\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل اکسل...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا): {sensor_columns}\")\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"هیچ ستون سنسوری پیدا نشد!\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد کل رکوردهای آماده: {len(X_scaled):,} رکورد\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(f\"\\nاجرای DBSCAN با eps={eps} و min_samples={min_samples}...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- تحلیل خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "n_total = len(labels)\n",
    "\n",
    "# جدا کردن نویز و خوشه‌ها\n",
    "noise_count = label_counts.get(-1, 0)\n",
    "cluster_labels = [lbl for lbl in label_counts.keys() if lbl != -1]\n",
    "cluster_sizes = [(lbl, label_counts[lbl]) for lbl in cluster_labels]\n",
    "\n",
    "# خوشه‌های بزرگ (عادی) و خوشه‌های کوچک (انومالی)\n",
    "large_clusters = [ (lbl, cnt) for lbl, cnt in cluster_sizes if cnt >= MIN_CLUSTER_SIZE ]\n",
    "small_clusters = [ (lbl, cnt) for lbl, cnt in cluster_sizes if cnt < MIN_CLUSTER_SIZE ]\n",
    "\n",
    "n_large_clusters = len(large_clusters)\n",
    "n_small_clusters = len(small_clusters)\n",
    "small_cluster_points = sum(cnt for _, cnt in small_clusters)\n",
    "\n",
    "# مجموع انومالی‌ها = نویز + خوشه‌های خیلی کوچک\n",
    "total_anomalies = noise_count + small_cluster_points\n",
    "\n",
    "# ------------------- نمایش خلاصه -------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                  خلاصه نتایج نهایی (با قوانین جدید)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'کل داده‌ها':<30}: {n_total:,}\")\n",
    "print(f\"{'نویز DBSCAN (برچسب -1)':<30}: {noise_count:,} رکورد\")\n",
    "print(f\"{'خوشه‌های کوچک (< {MIN_CLUSTER_SIZE} عضو)':<30}: {n_small_clusters} خوشه → {small_cluster_points:,} رکورد\")\n",
    "print(f\"{'مجموع انومالی‌ها':<30}: {total_anomalies:,} رکورد ({total_anomalies/n_total*100:5.2f}%)\")\n",
    "print(f\"{'خوشه‌های اصلی (عادی ≥ {MIN_CLUSTER_SIZE} عضو)':<30}: {n_large_clusters} خوشه\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# نمایش خوشه‌های اصلی (عادی)\n",
    "if n_large_clusters > 0:\n",
    "    print(f\"{'خوشه اصلی':<12} {'تعداد اعضا':<15} {'درصد از کل'}\")\n",
    "    print(\"-\"*70)\n",
    "    large_clusters_sorted = sorted(large_clusters, key=lambda x: x[1], reverse=True)\n",
    "    for i, (lbl, cnt) in enumerate(large_clusters_sorted, 1):\n",
    "        perc = cnt / n_total * 100\n",
    "        print(f\"خوشه {lbl:<8} {cnt:>10,} رکورد    ({perc:6.2f}%)\")\n",
    "        if i >= 15 and len(large_clusters_sorted) > 15:\n",
    "            print(f\"    ... و {len(large_clusters_sorted) - 15} خوشه دیگر\")\n",
    "            break\n",
    "else:\n",
    "    print(\"هیچ خوشه اصلی (عادی) پیدا نشد!\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها (نویز + خوشه‌های کوچک) -------------------\n",
    "# ایندکس‌هایی که انومالی هستند\n",
    "anomaly_mask = np.isin(labels, [-1] + [lbl for lbl, cnt in small_clusters])  # نویز + خوشه‌های کوچک\n",
    "anomaly_indices = df.index[anomaly_mask]\n",
    "\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "\n",
    "# اضافه کردن دلیل انومالی بودن\n",
    "anomalies_df['DBSCAN_Label'] = labels[anomaly_mask]\n",
    "anomalies_df['Anomaly_Reason'] = anomalies_df['DBSCAN_Label'].apply(\n",
    "    lambda x: 'Noise (-1)' if x == -1 else f'Small Cluster (size={label_counts[x]})'\n",
    ")\n",
    "\n",
    "# مرتب‌سازی بر اساس زمان\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n",
    "else:\n",
    "    anomalies_df = anomalies_df.sort_index()\n",
    "\n",
    "# ------------------- ذخیره انومالی‌ها -------------------\n",
    "anomalies_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\nفایل انومالی‌ها با موفقیت ذخیره شد!\")\n",
    "print(f\"   تعداد رکوردهای انومالی (شامل نویز و خوشه‌های کوچک): {len(anomalies_df):,} رکورد\")\n",
    "print(f\"   مسیر فایل: {output_file}\")\n",
    "print(\"\\nکار با موفقیت به پایان رسید!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن داده‌ها...\n",
      "ستون‌های سنسور: 7 تا\n",
      "اجرای DBSCAN...\n",
      "محاسبه شدت انومالی (model_value)...\n",
      "در حال ذخیره فایل کامل مدل...\n",
      "\n",
      "======================================================================\n",
      "                  نتایج نهایی مدل DBSCAN\n",
      "======================================================================\n",
      "کل رکوردها            : 10,927\n",
      "داده‌های عادی (Normal) : 10,414\n",
      "داده‌های غیرعادی (Anomaly): 513 (4.69%)\n",
      "بیشترین امتیاز انومالی: 100.0\n",
      "میانگین امتیاز انومالی در داده‌های غیرعادی: 70.5\n",
      "======================================================================\n",
      "فایل کامل مدل ذخیره شد → C:\\BI\\dbscan_model1.xlsx\n",
      "فایل فقط انومالی‌ها → C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n",
      "\n",
      "موفقیت‌آمیز! حالا می‌تونی این فایل رو مستقیم در Power BI لود کنی و داشبورد بسازی\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_anomalies = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "output_full_model = os.path.join(base_path, \"dbscan_model1.xlsx\")\n",
    "\n",
    "MIN_CLUSTER_SIZE = 15  # خوشه‌های کوچکتر از این → انومالی\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن داده‌ها...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and str(col.split('_')[-1]).isdigit()]\n",
    "print(f\"ستون‌های سنسور: {len(sensor_columns)} تا\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "optional_cols = ['unitID', 'TimeStamps', 'created_at', 'updated_at']\n",
    "available_optional = [col for col in optional_cols if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------- DBSCAN -------------------\n",
    "print(\"اجرای DBSCAN...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- تحلیل خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "large_cluster_labels = [lbl for lbl, cnt in label_counts.items() if lbl != -1 and cnt >= MIN_CLUSTER_SIZE]\n",
    "small_cluster_labels = [lbl for lbl, cnt in label_counts.items() if lbl != -1 and cnt < MIN_CLUSTER_SIZE]\n",
    "noise_mask = labels == -1\n",
    "\n",
    "# نقاط متعلق به خوشه‌های بزرگ (برای محاسبه فاصله)\n",
    "large_cluster_mask = np.isin(labels, large_cluster_labels)\n",
    "X_large = X_scaled[large_cluster_mask]\n",
    "\n",
    "# ------------------- محاسبه فاصله تا نزدیک‌ترین خوشه اصلی -------------------\n",
    "print(\"محاسبه شدت انومالی (model_value)...\")\n",
    "if len(X_large) > 0:\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X_large)\n",
    "    distances, _ = nbrs.kneighbors(X_scaled)\n",
    "    distances = distances.flatten()\n",
    "else:\n",
    "    distances = np.zeros(len(X_scaled))  # اگر هیچ خوشه بزرگی نبود\n",
    "\n",
    "# ------------------- ساخت model_value (0 تا 100) -------------------\n",
    "anomaly_score = np.zeros(len(df))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label in large_cluster_labels:\n",
    "        score = 0  # کاملاً نرمال\n",
    "    else:\n",
    "        # پایه: فاصله زیاد = امتیاز بالا\n",
    "        dist_score = min(distances[i] / (eps * 3), 1.0) * 70  # حداکثر 70 از فاصله\n",
    "        \n",
    "        if label == -1:\n",
    "            cluster_penalty = 30  # نویز خیلی غیرعادی\n",
    "        elif label in small_cluster_labels:\n",
    "            size = label_counts[label]\n",
    "            cluster_penalty = max(10, 30 - size)  # خوشه 1 نفره → 29، خوشه 14 نفره → 16\n",
    "        else:\n",
    "            cluster_penalty = 0\n",
    "        \n",
    "        score = dist_score + cluster_penalty\n",
    "        score = min(score, 100)  # حداکثر 100\n",
    "    \n",
    "    anomaly_score[i] = round(score, 2)\n",
    "\n",
    "# ------------------- ساخت دیتافریم نهایی -------------------\n",
    "result_df = df.copy()\n",
    "\n",
    "# ستون DateTime\n",
    "if 'RecordDate' in result_df.columns and 'RecordTime' in result_df.columns:\n",
    "    result_df['DateTime'] = pd.to_datetime(\n",
    "        result_df['RecordDate'].astype(str) + ' ' + result_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "else:\n",
    "    result_df['DateTime'] = pd.NaT\n",
    "\n",
    "# اضافه کردن ستون‌های مدل\n",
    "result_df['model_name'] = 'DBSCAN_Anomaly_Detection'\n",
    "result_df['model_result'] = np.where(anomaly_score == 0, 'Normal', 'Anomaly')\n",
    "result_df['model_value'] = anomaly_score\n",
    "\n",
    "# مرتب‌سازی ستون‌ها به ترتیب خواسته شده\n",
    "desired_columns = [\n",
    "    'id', 'AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344',\n",
    "    'AssetID_8346', 'AssetID_9286', 'AssetID_9287',\n",
    "    'unitID', 'model_name', 'model_result', 'model_value',\n",
    "    'DateTime', 'RecordDate', 'RecordTime', 'TimeStamps', 'created_at', 'updated_at'\n",
    "]\n",
    "\n",
    "# فقط ستون‌های موجود را نگه دار\n",
    "final_columns = [col for col in desired_columns if col in result_df.columns]\n",
    "# بقیه ستون‌های سنسور رو هم اضافه کن اگر نبودن\n",
    "for col in sensor_columns:\n",
    "    if col not in final_columns and col in result_df.columns:\n",
    "        final_columns.insert(final_columns.index('unitID') if 'unitID' in final_columns else 8, col)\n",
    "\n",
    "result_df = result_df[final_columns]\n",
    "\n",
    "# مرتب‌سازی بر اساس زمان\n",
    "result_df = result_df.sort_values('DateTime').reset_index(drop=True)\n",
    "\n",
    "# ------------------- ذخیره فایل کامل مدل -------------------\n",
    "print(\"در حال ذخیره فایل کامل مدل...\")\n",
    "result_df.to_excel(output_full_model, index=False)\n",
    "\n",
    "# ------------------- ذخیره فقط انومالی‌ها (مثل قبل) -------------------\n",
    "anomaly_mask_final = anomaly_score > 0\n",
    "anomalies_df = result_df[anomaly_mask_final].copy()\n",
    "anomalies_df.to_excel(output_anomalies, index=False)\n",
    "\n",
    "# ------------------- خلاصه نهایی -------------------\n",
    "n_anomalies = len(anomalies_df)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                  نتایج نهایی مدل DBSCAN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"کل رکوردها            : {len(result_df):,}\")\n",
    "print(f\"داده‌های عادی (Normal) : {len(result_df) - n_anomalies:,}\")\n",
    "print(f\"داده‌های غیرعادی (Anomaly): {n_anomalies:,} ({n_anomalies/len(result_df)*100:.2f}%)\")\n",
    "print(f\"بیشترین امتیاز انومالی: {anomaly_score.max():.1f}\")\n",
    "print(f\"میانگین امتیاز انومالی در داده‌های غیرعادی: {anomaly_score[anomaly_score > 0].mean():.1f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"فایل کامل مدل ذخیره شد → {output_full_model}\")\n",
    "print(f\"فایل فقط انومالی‌ها → {output_anomalies}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
