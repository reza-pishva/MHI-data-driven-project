{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل از مسیر:\n",
      "C:\\BI\\lube_oil_system_data_g11.xlsx\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا):\n",
      "['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد رکوردهای آماده برای مدل: 10927\n",
      "در حال اجرای الگوریتم DBSCAN...\n",
      "تعداد خوشه‌های تشکیل شده: 13\n",
      "تعداد انومالی‌های تشخیص داده شده: 466 رکورد (4.26%)\n",
      "\n",
      "در حال ذخیره فایل انومالی‌ها...\n",
      "انومالی‌ها با موفقیت ذخیره شدند!\n",
      "تعداد انومالی‌ها: 466 ردیف\n",
      "مسیر فایل خروجی:\n",
      "C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"   # مسیر اصلی\n",
    "\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "# بررسی وجود فایل ورودی\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل پیدا نشد!\\nمسیر مورد نظر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5           # می‌تونی بعداً بین 0.3 تا 1.0 تست کنی\n",
    "min_samples = 10    # حداقل نقاط برای تشکیل هسته خوشه\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل از مسیر:\")\n",
    "print(input_file)\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# پیدا کردن ستون‌های سنسور (آن‌هایی که با AssetID_ شروع می‌شن و با عدد تمام می‌شن)\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا):\")\n",
    "print(sensor_columns)\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"هیچ ستون سنسوری با نام AssetID_XXXX پیدا نشد!\")\n",
    "\n",
    "# ستون‌های شناسه و زمانی که می‌خواهیم در خروجی نگه داریمid, RecordDate, RecordTime\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش داده‌های سنسور -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "\n",
    "# پر کردن مقادیر گمشده (DBSCAN با NaN کار نمی‌کنه)\n",
    "X = X.fillna(X.mean())        \n",
    "\n",
    "# استانداردسازی (خیلی مهم برای DBSCAN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد رکوردهای آماده برای مدل: {X_scaled.shape[0]}\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(\"در حال اجرای الگوریتم DBSCAN...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# آمار سریع\n",
    "n_anomalies = np.sum(labels == -1)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(f\"تعداد خوشه‌های تشکیل شده: {n_clusters}\")\n",
    "print(f\"تعداد انومالی‌های تشخیص داده شده: {n_anomalies} رکورد ({n_anomalies/len(labels)*100:.2f}%)\")\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها -------------------\n",
    "anomaly_indices = df.index[labels == -1]  # ایندکس‌های اصلی در دیتافریم اصلی\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "\n",
    "# اضافه کردن برچسب DBSCAN (همه -1 هستند)\n",
    "anomalies_df['DBSCAN_Label'] = -1\n",
    "\n",
    "# مرتب‌سازی بر اساس تاریخ و زمان (اگر موجود باشد)\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n",
    "else:\n",
    "    anomalies_df = anomalies_df.sort_index()\n",
    "\n",
    "# ------------------- ذخیره فایل انومالی در مسیر C:\\BI -------------------\n",
    "print(\"\\nدر حال ذخیره فایل انومالی‌ها...\")\n",
    "anomalies_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"انومالی‌ها با موفقیت ذخیره شدند!\")\n",
    "print(f\"تعداد انومالی‌ها: {len(anomalies_df)} ردیف\")\n",
    "print(f\"مسیر فایل خروجی:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل اکسل...\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا): ['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد کل رکوردهای آماده برای خوشه‌بندی: 10,927 رکورد\n",
      "\n",
      "اجرای DBSCAN با eps=0.5 و min_samples=10...\n",
      "\n",
      "============================================================\n",
      "               خلاصه نتایج خوشه‌بندی DBSCAN\n",
      "============================================================\n",
      "تعداد کل رکورد ها        : 10,927\n",
      "تعداد انومالی (نویز)     : 466  (  4.26%)\n",
      "تعداد خوشه‌های اصلی      : 13\n",
      "------------------------------------------------------------\n",
      "خوشه       تعداد اعضا      درصد از کل\n",
      "------------------------------------------------------------\n",
      "خوشه 0           4,318 رکورد    ( 39.52%)\n",
      "خوشه 6           2,791 رکورد    ( 25.54%)\n",
      "خوشه 12          1,810 رکورد    ( 16.56%)\n",
      "خوشه 1           1,390 رکورد    ( 12.72%)\n",
      "خوشه 9              29 رکورد    (  0.27%)\n",
      "خوشه 2              25 رکورد    (  0.23%)\n",
      "خوشه 4              19 رکورد    (  0.17%)\n",
      "خوشه 7              16 رکورد    (  0.15%)\n",
      "خوشه 11             16 رکورد    (  0.15%)\n",
      "خوشه 8              14 رکورد    (  0.13%)\n",
      "خوشه 5              13 رکورد    (  0.12%)\n",
      "خوشه 10             13 رکورد    (  0.12%)\n",
      "خوشه 3               7 رکورد    (  0.06%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "# بررسی وجود فایل ورودی\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل ورودی پیدا نشد!\\nمسیر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل اکسل...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا): {sensor_columns}\")\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"ستون سنسوری پیدا نشد!\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد کل رکوردهای آماده برای خوشه‌بندی: {len(X_scaled):,} رکورد\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(f\"\\nاجرای DBSCAN با eps={eps} و min_samples={min_samples}...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- آمار خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "n_clusters = len(label_counts) - (1 if -1 in label_counts else 0)\n",
    "n_noise = label_counts[-1] if -1 in label_counts else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"               خلاصه نتایج خوشه‌بندی DBSCAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'تعداد کل رکورد ها':<25}: {len(labels):,}\")\n",
    "print(f\"{'تعداد انومالی (نویز)':<25}: {n_noise:,}  ({n_noise/len(labels)*100:6.2f}%)\")\n",
    "print(f\"{'تعداد خوشه‌های اصلی':<25}: {n_clusters:,}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# نمایش تعداد اعضا در هر خوشه (به ترتیب اندازه)\n",
    "cluster_sizes = []\n",
    "for label, count in label_counts.items():\n",
    "    if label != -1:  # فقط خوشه‌های واقعی\n",
    "        cluster_sizes.append((label, count))\n",
    "\n",
    "# مرتب‌سازی بر اساس تعداد اعضا (نزولی)\n",
    "cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'خوشه':<10} {'تعداد اعضا':<15} {'درصد از کل'}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (label, count) in enumerate(cluster_sizes, 1):\n",
    "    percentage = count / len(labels) * 100\n",
    "    print(f\"خوشه {label:<6} {count:>10,} رکورد    ({percentage:6.2f}%)\")\n",
    "    if i >= 20:  # فقط 20 خوشه بزرگ را نشان بده تا شلوغ نشود\n",
    "        if len(cluster_sizes) > 20:\n",
    "            print(f\"    ... و {len(cluster_sizes) - 20} خوشه کوچک دیگر\")\n",
    "        break\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها -------------------\n",
    "anomaly_mask = labels == -1\n",
    "anomaly_indices = df.index[anomaly_mask]\n",
    "\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "anomalies_df['DBSCAN_Label'] = -1\n",
    "anomalies_df['Cluster_Size'] = n_noise  # فقط برای اطلاعات\n",
    "\n",
    "# مرتب‌سازی بر اساس تاریخ و زمان\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن فایل اکسل...\n",
      "\n",
      "ستون‌های سنسور شناسایی شده (7 تا): ['AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344', 'AssetID_8346', 'AssetID_9286', 'AssetID_9287']\n",
      "\n",
      "تعداد کل رکوردهای آماده: 10,927 رکورد\n",
      "\n",
      "اجرای DBSCAN با eps=0.5 و min_samples=10...\n",
      "\n",
      "======================================================================\n",
      "                  خلاصه نتایج نهایی (با قوانین جدید)\n",
      "======================================================================\n",
      "کل داده‌ها                    : 10,927\n",
      "نویز DBSCAN (برچسب -1)        : 466 رکورد\n",
      "خوشه‌های کوچک (< {MIN_CLUSTER_SIZE} عضو): 4 خوشه → 47 رکورد\n",
      "مجموع انومالی‌ها              : 513 رکورد ( 4.69%)\n",
      "خوشه‌های اصلی (عادی ≥ {MIN_CLUSTER_SIZE} عضو): 9 خوشه\n",
      "----------------------------------------------------------------------\n",
      "خوشه اصلی    تعداد اعضا      درصد از کل\n",
      "----------------------------------------------------------------------\n",
      "خوشه 0             4,318 رکورد    ( 39.52%)\n",
      "خوشه 6             2,791 رکورد    ( 25.54%)\n",
      "خوشه 12            1,810 رکورد    ( 16.56%)\n",
      "خوشه 1             1,390 رکورد    ( 12.72%)\n",
      "خوشه 9                29 رکورد    (  0.27%)\n",
      "خوشه 2                25 رکورد    (  0.23%)\n",
      "خوشه 4                19 رکورد    (  0.17%)\n",
      "خوشه 7                16 رکورد    (  0.15%)\n",
      "خوشه 11               16 رکورد    (  0.15%)\n",
      "======================================================================\n",
      "\n",
      "فایل انومالی‌ها با موفقیت ذخیره شد!\n",
      "   تعداد رکوردهای انومالی (شامل نویز و خوشه‌های کوچک): 513 رکورد\n",
      "   مسیر فایل: C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n",
      "\n",
      "کار با موفقیت به پایان رسید!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل ورودی پیدا نشد!\\nمسیر: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "MIN_CLUSTER_SIZE = 15  # حداقل اندازه خوشه برای \"عادی\" در نظر گرفتن\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن فایل اکسل...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "print(f\"\\nستون‌های سنسور شناسایی شده ({len(sensor_columns)} تا): {sensor_columns}\")\n",
    "\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"هیچ ستون سنسوری پیدا نشد!\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "available_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nتعداد کل رکوردهای آماده: {len(X_scaled):,} رکورد\")\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "print(f\"\\nاجرای DBSCAN با eps={eps} و min_samples={min_samples}...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- تحلیل خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "n_total = len(labels)\n",
    "\n",
    "# جدا کردن نویز و خوشه‌ها\n",
    "noise_count = label_counts.get(-1, 0)\n",
    "cluster_labels = [lbl for lbl in label_counts.keys() if lbl != -1]\n",
    "cluster_sizes = [(lbl, label_counts[lbl]) for lbl in cluster_labels]\n",
    "\n",
    "# خوشه‌های بزرگ (عادی) و خوشه‌های کوچک (انومالی)\n",
    "large_clusters = [ (lbl, cnt) for lbl, cnt in cluster_sizes if cnt >= MIN_CLUSTER_SIZE ]\n",
    "small_clusters = [ (lbl, cnt) for lbl, cnt in cluster_sizes if cnt < MIN_CLUSTER_SIZE ]\n",
    "\n",
    "n_large_clusters = len(large_clusters)\n",
    "n_small_clusters = len(small_clusters)\n",
    "small_cluster_points = sum(cnt for _, cnt in small_clusters)\n",
    "\n",
    "# مجموع انومالی‌ها = نویز + خوشه‌های خیلی کوچک\n",
    "total_anomalies = noise_count + small_cluster_points\n",
    "\n",
    "# ------------------- نمایش خلاصه -------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                  خلاصه نتایج نهایی (با قوانین جدید)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'کل داده‌ها':<30}: {n_total:,}\")\n",
    "print(f\"{'نویز DBSCAN (برچسب -1)':<30}: {noise_count:,} رکورد\")\n",
    "print(f\"{'خوشه‌های کوچک (< {MIN_CLUSTER_SIZE} عضو)':<30}: {n_small_clusters} خوشه → {small_cluster_points:,} رکورد\")\n",
    "print(f\"{'مجموع انومالی‌ها':<30}: {total_anomalies:,} رکورد ({total_anomalies/n_total*100:5.2f}%)\")\n",
    "print(f\"{'خوشه‌های اصلی (عادی ≥ {MIN_CLUSTER_SIZE} عضو)':<30}: {n_large_clusters} خوشه\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# نمایش خوشه‌های اصلی (عادی)\n",
    "if n_large_clusters > 0:\n",
    "    print(f\"{'خوشه اصلی':<12} {'تعداد اعضا':<15} {'درصد از کل'}\")\n",
    "    print(\"-\"*70)\n",
    "    large_clusters_sorted = sorted(large_clusters, key=lambda x: x[1], reverse=True)\n",
    "    for i, (lbl, cnt) in enumerate(large_clusters_sorted, 1):\n",
    "        perc = cnt / n_total * 100\n",
    "        print(f\"خوشه {lbl:<8} {cnt:>10,} رکورد    ({perc:6.2f}%)\")\n",
    "        if i >= 15 and len(large_clusters_sorted) > 15:\n",
    "            print(f\"    ... و {len(large_clusters_sorted) - 15} خوشه دیگر\")\n",
    "            break\n",
    "else:\n",
    "    print(\"هیچ خوشه اصلی (عادی) پیدا نشد!\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ------------------- استخراج انومالی‌ها (نویز + خوشه‌های کوچک) -------------------\n",
    "# ایندکس‌هایی که انومالی هستند\n",
    "anomaly_mask = np.isin(labels, [-1] + [lbl for lbl, cnt in small_clusters])  # نویز + خوشه‌های کوچک\n",
    "anomaly_indices = df.index[anomaly_mask]\n",
    "\n",
    "anomalies_df = df.loc[anomaly_indices, available_id_cols + sensor_columns].copy()\n",
    "\n",
    "# اضافه کردن دلیل انومالی بودن\n",
    "anomalies_df['DBSCAN_Label'] = labels[anomaly_mask]\n",
    "anomalies_df['Anomaly_Reason'] = anomalies_df['DBSCAN_Label'].apply(\n",
    "    lambda x: 'Noise (-1)' if x == -1 else f'Small Cluster (size={label_counts[x]})'\n",
    ")\n",
    "\n",
    "# مرتب‌سازی بر اساس زمان\n",
    "if 'RecordDate' in anomalies_df.columns and 'RecordTime' in anomalies_df.columns:\n",
    "    anomalies_df['datetime_temp'] = pd.to_datetime(\n",
    "        anomalies_df['RecordDate'].astype(str) + ' ' + anomalies_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    anomalies_df = anomalies_df.sort_values('datetime_temp').drop(columns='datetime_temp')\n",
    "else:\n",
    "    anomalies_df = anomalies_df.sort_index()\n",
    "\n",
    "# ------------------- ذخیره انومالی‌ها -------------------\n",
    "anomalies_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\nفایل انومالی‌ها با موفقیت ذخیره شد!\")\n",
    "print(f\"   تعداد رکوردهای انومالی (شامل نویز و خوشه‌های کوچک): {len(anomalies_df):,} رکورد\")\n",
    "print(f\"   مسیر فایل: {output_file}\")\n",
    "print(\"\\nکار با موفقیت به پایان رسید!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن داده‌ها...\n",
      "ستون‌های سنسور: 7 تا\n",
      "اجرای DBSCAN...\n",
      "محاسبه شدت انومالی (model_value)...\n",
      "در حال ذخیره فایل کامل مدل...\n",
      "\n",
      "======================================================================\n",
      "                  نتایج نهایی مدل DBSCAN\n",
      "======================================================================\n",
      "کل رکوردها            : 10,927\n",
      "داده‌های عادی (Normal) : 10,414\n",
      "داده‌های غیرعادی (Anomaly): 513 (4.69%)\n",
      "بیشترین امتیاز انومالی: 100.0\n",
      "میانگین امتیاز انومالی در داده‌های غیرعادی: 70.5\n",
      "======================================================================\n",
      "فایل کامل مدل ذخیره شد → C:\\BI\\dbscan_model1.xlsx\n",
      "فایل فقط انومالی‌ها → C:\\BI\\lube_oil_system_anomalies_g11.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------- تنظیمات -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_anomalies = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "output_full_model = os.path.join(base_path, \"dbscan_model1.xlsx\")\n",
    "\n",
    "MIN_CLUSTER_SIZE = 15  # خوشه‌های کوچکتر از این → انومالی\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"در حال خواندن داده‌ها...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and str(col.split('_')[-1]).isdigit()]\n",
    "print(f\"ستون‌های سنسور: {len(sensor_columns)} تا\")\n",
    "\n",
    "id_columns = ['id', 'RecordDate', 'RecordTime']\n",
    "optional_cols = ['unitID', 'TimeStamps', 'created_at', 'updated_at']\n",
    "available_optional = [col for col in optional_cols if col in df.columns]\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------- DBSCAN -------------------\n",
    "print(\"اجرای DBSCAN...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "# ------------------- تحلیل خوشه‌ها -------------------\n",
    "label_counts = Counter(labels)\n",
    "large_cluster_labels = [lbl for lbl, cnt in label_counts.items() if lbl != -1 and cnt >= MIN_CLUSTER_SIZE]\n",
    "small_cluster_labels = [lbl for lbl, cnt in label_counts.items() if lbl != -1 and cnt < MIN_CLUSTER_SIZE]\n",
    "noise_mask = labels == -1\n",
    "\n",
    "# نقاط متعلق به خوشه‌های بزرگ (برای محاسبه فاصله)\n",
    "large_cluster_mask = np.isin(labels, large_cluster_labels)\n",
    "X_large = X_scaled[large_cluster_mask]\n",
    "\n",
    "# ------------------- محاسبه فاصله تا نزدیک‌ترین خوشه اصلی -------------------\n",
    "print(\"محاسبه شدت انومالی (model_value)...\")\n",
    "if len(X_large) > 0:\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X_large)\n",
    "    distances, _ = nbrs.kneighbors(X_scaled)\n",
    "    distances = distances.flatten()\n",
    "else:\n",
    "    distances = np.zeros(len(X_scaled))  # اگر هیچ خوشه بزرگی نبود\n",
    "\n",
    "# ------------------- ساخت model_value (0 تا 100) -------------------\n",
    "anomaly_score = np.zeros(len(df))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label in large_cluster_labels:\n",
    "        score = 0  # کاملاً نرمال\n",
    "    else:\n",
    "        # پایه: فاصله زیاد = امتیاز بالا\n",
    "        dist_score = min(distances[i] / (eps * 3), 1.0) * 70  # حداکثر 70 از فاصله\n",
    "        \n",
    "        if label == -1:\n",
    "            cluster_penalty = 30  # نویز خیلی غیرعادی\n",
    "        elif label in small_cluster_labels:\n",
    "            size = label_counts[label]\n",
    "            cluster_penalty = max(10, 30 - size)  # خوشه 1 نفره → 29، خوشه 14 نفره → 16\n",
    "        else:\n",
    "            cluster_penalty = 0\n",
    "        \n",
    "        score = dist_score + cluster_penalty\n",
    "        score = min(score, 100)  # حداکثر 100\n",
    "    \n",
    "    anomaly_score[i] = round(score, 2)\n",
    "\n",
    "# ------------------- ساخت دیتافریم نهایی -------------------\n",
    "result_df = df.copy()\n",
    "\n",
    "# ستون DateTime\n",
    "if 'RecordDate' in result_df.columns and 'RecordTime' in result_df.columns:\n",
    "    result_df['DateTime'] = pd.to_datetime(\n",
    "        result_df['RecordDate'].astype(str) + ' ' + result_df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "else:\n",
    "    result_df['DateTime'] = pd.NaT\n",
    "\n",
    "# اضافه کردن ستون‌های مدل\n",
    "result_df['model_name'] = 'DBSCAN_Anomaly_Detection'\n",
    "result_df['model_result'] = np.where(anomaly_score == 0, 'Normal', 'Anomaly')\n",
    "result_df['model_value'] = anomaly_score\n",
    "\n",
    "# مرتب‌سازی ستون‌ها به ترتیب خواسته شده\n",
    "desired_columns = [\n",
    "    'id', 'AssetID_8341', 'AssetID_8342', 'AssetID_8343', 'AssetID_8344',\n",
    "    'AssetID_8346', 'AssetID_9286', 'AssetID_9287',\n",
    "    'unitID', 'model_name', 'model_result', 'model_value',\n",
    "    'DateTime', 'RecordDate', 'RecordTime', 'TimeStamps', 'created_at', 'updated_at'\n",
    "]\n",
    "\n",
    "# فقط ستون‌های موجود را نگه دار\n",
    "final_columns = [col for col in desired_columns if col in result_df.columns]\n",
    "# بقیه ستون‌های سنسور رو هم اضافه کن اگر نبودن\n",
    "for col in sensor_columns:\n",
    "    if col not in final_columns and col in result_df.columns:\n",
    "        final_columns.insert(final_columns.index('unitID') if 'unitID' in final_columns else 8, col)\n",
    "\n",
    "result_df = result_df[final_columns]\n",
    "\n",
    "# مرتب‌سازی بر اساس زمان\n",
    "result_df = result_df.sort_values('DateTime').reset_index(drop=True)\n",
    "\n",
    "# ------------------- ذخیره فایل کامل مدل -------------------\n",
    "print(\"در حال ذخیره فایل کامل مدل...\")\n",
    "result_df.to_excel(output_full_model, index=False)\n",
    "\n",
    "# ------------------- ذخیره فقط انومالی‌ها (مثل قبل) -------------------\n",
    "anomaly_mask_final = anomaly_score > 0\n",
    "anomalies_df = result_df[anomaly_mask_final].copy()\n",
    "anomalies_df.to_excel(output_anomalies, index=False)\n",
    "\n",
    "# ------------------- خلاصه نهایی -------------------\n",
    "n_anomalies = len(anomalies_df)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                  نتایج نهایی مدل DBSCAN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"کل رکوردها            : {len(result_df):,}\")\n",
    "print(f\"داده‌های عادی (Normal) : {len(result_df) - n_anomalies:,}\")\n",
    "print(f\"داده‌های غیرعادی (Anomaly): {n_anomalies:,} ({n_anomalies/len(result_df)*100:.2f}%)\")\n",
    "print(f\"بیشترین امتیاز انومالی: {anomaly_score.max():.1f}\")\n",
    "print(f\"میانگین امتیاز انومالی در داده‌های غیرعادی: {anomaly_score[anomaly_score > 0].mean():.1f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"فایل کامل مدل ذخیره شد → {output_full_model}\")\n",
    "print(f\"فایل فقط انومالی‌ها → {output_anomalies}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شروع تحلیل در: 2025-11-25 15:47:43\n",
      "استخراج انومالی‌های 100 روز گذشته: از 2025-08-17 تا 2025-11-25\n",
      "فایل انومالی‌ها با نام ثابت ذخیره می‌شود: C:\\BI\\dbscan_lube_oil_system_anomalies_g11_1.xlsx\n",
      "فایل کامل نتایج (آرشیو): C:\\BI\\DBSCAN_Full_Result_20251125_154742.xlsx\n",
      "\n",
      "در حال خواندن داده‌ها...\n",
      "تعداد کل رکوردها: 10,927\n",
      "تعداد سنسورها: 7\n",
      "تعداد رکورد با تاریخ معتبر: 10,927\n",
      "اجرای DBSCAN (eps=0.5, min_samples=10)...\n",
      "محاسبه model_value با سیستم هوشمند...\n",
      "\n",
      "ذخیره فایل کامل نتایج...\n",
      "فیلتر انومالی‌های 100 روز گذشته...\n",
      "تعداد انومالی در 100 روز گذشته: 21 رکورد\n",
      "\n",
      "==========================================================================================\n",
      "                   گزارش نهایی – انومالی‌های 100 روز گذشته\n",
      "==========================================================================================\n",
      "بازه زمانی          : 2025-08-17 → 2025-11-25\n",
      "تعداد انومالی یافت شده : 21 رکورد\n",
      "بالاترین امتیاز       : 100.00\n",
      "میانگین امتیاز        : 71.37\n",
      "آخرین انومالی        : 2025-08-17 08:49:16\n",
      "==========================================================================================\n",
      "فایل انومالی‌ها (100 روز اخیر) → C:\\BI\\dbscan_lube_oil_system_anomalies_g11_1.xlsx\n",
      "فایل کامل نتایج (آرشیو)        → C:\\BI\\DBSCAN_Full_Result_20251125_154742.xlsx\n",
      "==========================================================================================\n",
      "تحلیل با موفقیت انجام شد! فایل lube_oil_system_anomalies_g11.xlsx به‌روزرسانی شد.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ------------------- تنظیمات مسیر و نام فایل -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "\n",
    "# فایل ورودی\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "\n",
    "# فایل خروجی انومالی‌ها → دقیقاً نام ثابت (مثل قبل)\n",
    "output_anomalies = os.path.join(base_path, \"dbscan_lube_oil_system_anomalies_g11_1.xlsx\")\n",
    "\n",
    "# فایل کامل نتایج → با تاریخ برای آرشیو\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_full_model = os.path.join(base_path, f\"DBSCAN_Full_Result_{timestamp}.xlsx\")\n",
    "\n",
    "# تاریخ امروز و 100 روز قبل\n",
    "today = pd.Timestamp.today().normalize()\n",
    "days_ago_100 = today - timedelta(days=100)\n",
    "\n",
    "print(f\"شروع تحلیل در: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"استخراج انومالی‌های 100 روز گذشته: از {days_ago_100.strftime('%Y-%m-%d')} تا {today.strftime('%Y-%m-%d')}\")\n",
    "print(f\"فایل انومالی‌ها با نام ثابت ذخیره می‌شود: {output_anomalies}\")\n",
    "print(f\"فایل کامل نتایج (آرشیو): {output_full_model}\")\n",
    "\n",
    "# ------------------- پارامترهای مدل -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "MIN_CLUSTER_SIZE = 15\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "print(\"\\nدر حال خواندن داده‌ها...\")\n",
    "df = pd.read_excel(input_file)\n",
    "print(f\"تعداد کل رکوردها: {len(df):,}\")\n",
    "\n",
    "# تشخیص ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and str(col.split('_')[-1]).isdigit()]\n",
    "print(f\"تعداد سنسورها: {len(sensor_columns)}\")\n",
    "\n",
    "# ------------------- ساخت ستون DateTime -------------------\n",
    "if 'RecordDate' in df.columns and 'RecordTime' in df.columns:\n",
    "    df['DateTime'] = pd.to_datetime(\n",
    "        df['RecordDate'].astype(str) + ' ' + df['RecordTime'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "df = df.dropna(subset=['DateTime']).copy()\n",
    "df['DateOnly'] = df['DateTime'].dt.normalize()\n",
    "\n",
    "print(f\"تعداد رکورد با تاریخ معتبر: {len(df):,}\")\n",
    "\n",
    "# ------------------- پیش‌پردازش و DBSCAN -------------------\n",
    "X = df[sensor_columns].fillna(df[sensor_columns].mean())\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"اجرای DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "labels = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1).fit_predict(X_scaled)\n",
    "\n",
    "# تحلیل خوشه‌ها\n",
    "label_counts = Counter(labels)\n",
    "large_clusters = [lbl for lbl, cnt in label_counts.items() if lbl != -1 and cnt >= MIN_CLUSTER_SIZE]\n",
    "\n",
    "# فاصله تا نزدیک‌ترین خوشه بزرگ\n",
    "large_mask = np.isin(labels, large_clusters)\n",
    "X_large = X_scaled[large_mask]\n",
    "if len(X_large) > 0:\n",
    "    distances = NearestNeighbors(n_neighbors=1).fit(X_large).kneighbors(X_scaled)[0].flatten()\n",
    "else:\n",
    "    distances = np.full(len(X_scaled), eps * 10)\n",
    "\n",
    "# ------------------- محاسبه امتیاز هوشمند و متنوع -------------------\n",
    "print(\"محاسبه model_value با سیستم هوشمند...\")\n",
    "anomaly_score = np.zeros(len(df))\n",
    "\n",
    "for i, (label, dist) in enumerate(zip(labels, distances)):\n",
    "    if label in large_clusters:\n",
    "        anomaly_score[i] = 0.0\n",
    "    else:\n",
    "        dist_ratio = min(dist / eps, 6.0)\n",
    "        base_score = (dist_ratio ** 2.7) * 12.5\n",
    "        \n",
    "        if label == -1:\n",
    "            penalty = 35 + (dist_ratio * 8)\n",
    "        else:\n",
    "            size = label_counts[label]\n",
    "            penalty = max(20, 48 - size * 2.2)\n",
    "        \n",
    "        score = base_score + penalty\n",
    "        anomaly_score[i] = round(min(score, 100), 2)\n",
    "\n",
    "# ------------------- ساخت دیتافریم نهایی -------------------\n",
    "result_df = df.copy()\n",
    "result_df['model_name'] = 'DBSCAN_Anomaly_SmartScore'\n",
    "result_df['model_result'] = np.where(anomaly_score == 0, 'Normal', 'Anomaly')\n",
    "result_df['model_value'] = anomaly_score\n",
    "result_df['cluster_label'] = labels\n",
    "\n",
    "# مرتب‌سازی ستون‌ها\n",
    "cols_order = ['id', 'unitID', 'model_name', 'model_result', 'model_value', 'cluster_label',\n",
    "              'DateTime', 'DateOnly', 'RecordDate', 'RecordTime']\n",
    "final_cols = [c for c in cols_order if c in result_df.columns]\n",
    "sensor_pos = final_cols.index('unitID') + 1 if 'unitID' in final_cols else 8\n",
    "for col in sensor_columns:\n",
    "    if col not in final_cols:\n",
    "        final_cols.insert(sensor_pos, col)\n",
    "        sensor_pos += 1\n",
    "for col in ['TimeStamps', 'created_at', 'updated_at']:\n",
    "    if col in result_df.columns and col not in final_cols:\n",
    "        final_cols.append(col)\n",
    "\n",
    "result_df = result_df[final_cols].sort_values('DateTime').reset_index(drop=True)\n",
    "\n",
    "# ------------------- ذخیره فایل کامل (آرشیو) -------------------\n",
    "print(f\"\\nذخیره فایل کامل نتایج...\")\n",
    "result_df.to_excel(output_full_model, index=False)\n",
    "\n",
    "# ------------------- استخراج و ذخیره فقط انومالی‌های 100 روز گذشته -------------------\n",
    "print(f\"فیلتر انومالی‌های 100 روز گذشته...\")\n",
    "anomalies_100days = result_df[\n",
    "    (result_df['model_value'] > 0) &\n",
    "    (result_df['DateOnly'] >= days_ago_100)\n",
    "].copy()\n",
    "\n",
    "anomalies_100days = anomalies_100days.sort_values('model_value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"تعداد انومالی در 100 روز گذشته: {len(anomalies_100days):,} رکورد\")\n",
    "\n",
    "# ذخیره با نام ثابت (دقیقاً همان نام قبلی)\n",
    "anomalies_100days.to_excel(output_anomalies, index=False)\n",
    "\n",
    "# ------------------- گزارش نهایی -------------------\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"                   گزارش نهایی – انومالی‌های 100 روز گذشته\")\n",
    "print(\"=\"*90)\n",
    "print(f\"بازه زمانی          : {days_ago_100.strftime('%Y-%m-%d')} → {today.strftime('%Y-%m-%d')}\")\n",
    "print(f\"تعداد انومالی یافت شده : {len(anomalies_100days):,} رکورد\")\n",
    "if len(anomalies_100days) > 0:\n",
    "    print(f\"بالاترین امتیاز       : {anomalies_100days['model_value'].iloc[0]:.2f}\")\n",
    "    print(f\"میانگین امتیاز        : {anomalies_100days['model_value'].mean():.2f}\")\n",
    "    print(f\"آخرین انومالی        : {anomalies_100days.iloc[0]['DateTime']}\")\n",
    "else:\n",
    "    print(\"هیچ انومالی در 100 روز گذشته یافت نشد.\")\n",
    "print(\"=\"*90)\n",
    "print(f\"فایل انومالی‌ها (100 روز اخیر) → {output_anomalies}\")\n",
    "print(f\"فایل کامل نتایج (آرشیو)        → {output_full_model}\")\n",
    "print(\"=\"*90)\n",
    "print(\"تحلیل با موفقیت انجام شد! فایل lube_oil_system_anomalies_g11.xlsx به‌روزرسانی شد.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "فایل distance_to_cluster_output.xlsx ساخته شد.\n",
      "مسیر: C:\\BI\\distance_to_cluster_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# ------------------- تنظیمات مسیر -------------------\n",
    "base_path = r\"C:\\BI\"\n",
    "input_file = os.path.join(base_path, \"lube_oil_system_data_g11.xlsx\")\n",
    "output_file_anomaly = os.path.join(base_path, \"lube_oil_system_anomalies_g11.xlsx\")\n",
    "output_file_distance = os.path.join(base_path, \"distance_to_cluster_output.xlsx\")\n",
    "\n",
    "# ------------------- بررسی فایل -------------------\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"فایل پیدا نشد!: {input_file}\")\n",
    "\n",
    "# ------------------- پارامترهای DBSCAN -------------------\n",
    "eps = 0.5\n",
    "min_samples = 10\n",
    "\n",
    "# ------------------- خواندن داده -------------------\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# شناسایی ستون‌های سنسور\n",
    "sensor_columns = [col for col in df.columns if col.startswith('AssetID_') and col.split('_')[-1].isdigit()]\n",
    "if len(sensor_columns) == 0:\n",
    "    raise ValueError(\"هیچ ستون AssetID_XXXX پیدا نشد.\")\n",
    "\n",
    "# ستون‌های تاریخ و ساعت\n",
    "date_col = \"RecordDate\"\n",
    "time_col = \"RecordTime\"\n",
    "\n",
    "# ------------------- پیش‌پردازش -------------------\n",
    "X = df[sensor_columns].copy()\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------- اجرای DBSCAN -------------------\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels = db.fit_predict(X_scaled)\n",
    "\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# ------------------- ذخیره انومالی‌ها -------------------\n",
    "anomalies_df = df[df[\"cluster\"] == -1].copy()\n",
    "anomalies_df[\"DBSCAN_Label\"] = -1\n",
    "anomalies_df.to_excel(output_file_anomaly, index=False)\n",
    "\n",
    "# ------------------- محاسبه مرکز کلاسترها -------------------\n",
    "unique_clusters = [c for c in set(labels) if c != -1]  # خوشه‌های نرمال\n",
    "\n",
    "centroids = {}\n",
    "for c in unique_clusters:\n",
    "    centroids[c] = X_scaled[df[\"cluster\"] == c].mean(axis=0)\n",
    "\n",
    "# ------------------- محاسبه فاصله هر نقطه تا مرکز کلاستر -------------------\n",
    "distances = []\n",
    "for i in range(len(df)):\n",
    "    c = df.loc[i, \"cluster\"]\n",
    "    if c == -1:\n",
    "        distances.append(np.nan)\n",
    "    else:\n",
    "        center = centroids[c]\n",
    "        point = X_scaled[i]\n",
    "        dist = np.linalg.norm(point - center)\n",
    "        distances.append(dist)\n",
    "\n",
    "df[\"distance_to_centroid\"] = distances\n",
    "\n",
    "# ------------------- انتخاب آخرین 400 رکورد -------------------\n",
    "df[\"datetime_temp\"] = pd.to_datetime(df[date_col].astype(str) + \" \" +\n",
    "                                     df[time_col].astype(str), errors=\"coerce\")\n",
    "\n",
    "df_sorted = df.sort_values(\"datetime_temp\").tail(400)\n",
    "\n",
    "# ------------------- ساخت خروجی فقط با: تاریخ – ساعت – فاصله -------------------\n",
    "output_df = df_sorted[[date_col, time_col, \"distance_to_centroid\"]]\n",
    "\n",
    "output_df.to_excel(output_file_distance, index=False)\n",
    "\n",
    "print(\"\\nفایل distance_to_cluster_output.xlsx ساخته شد.\")\n",
    "print(\"مسیر:\", output_file_distance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
